{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "id": "jQqlt9jQtSEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "xk01IV0xtjai"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Mistral-7B Results**"
      ],
      "metadata": {
        "id": "SiqnSaIjpVJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from torch.nn import DataParallel\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('NousResearch/Yarn-Mistral-7b-128k')\n",
        "model = AutoModelForCausalLM.from_pretrained('NousResearch/Yarn-Mistral-7b-128k')\n",
        "print(model)"
      ],
      "metadata": {
        "id": "rGkkfFZLmRlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mistal-7B (Zero-shot)**"
      ],
      "metadata": {
        "id": "4IlzjKlxYVPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/Test_modified_432.csv')\n",
        "\n",
        "llm_generated_templates = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    log_event = row['Content']\n",
        "    ground_truth = row['EventTemplate']\n",
        "\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "<|im_start|>system\n",
        "You are a log parsing assistant. Your task is to identify variable elements within the logs, generalize these elements, and construct a template that represents the structure of these log messages.\n",
        "<|im_end|>\n",
        "\n",
        "<|im_start|>user\n",
        "Extract the template for the following log message. Replace any variable element with the placeholder '<*>'. Do not provide any explanation, only return the template.\n",
        "```{log_event}```\n",
        "<|im_end|>\n",
        "\n",
        "<|im_start|>assistant\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=60,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    print(\"========================================\")\n",
        "    print(f\"index: {index}\")\n",
        "    print(f\"Whole Output: {output}\")\n",
        "    print(\"========================================\")\n",
        "\n",
        "    output = output.replace('\"', '')\n",
        "    output = output.replace('\\n', '')\n",
        "\n",
        "    after_assistant = output.split(\"assistant\")[-1]\n",
        "\n",
        "    if \"<|im_end|>\" in after_assistant:\n",
        "        parsed_output = after_assistant.split(\"<|im_end|>\")[0].strip()\n",
        "    else:\n",
        "        parsed_output = after_assistant.strip()\n",
        "\n",
        "    llm_generated_templates.append(parsed_output)\n",
        "    print(\"----------------------------------------------\")\n",
        "    print(f\"Log Event: {log_event}\")\n",
        "    print(f\"Ground Truth: {ground_truth}\")\n",
        "    print(f\"Extracted Template: {parsed_output}\")\n",
        "    print(\"----------------------------------------------\")\n",
        "\n",
        "df['ZsMistralExtractedTemplate'] = llm_generated_templates\n",
        "df.to_csv('/content/ZS_Test_modified_432.csv', index=False)"
      ],
      "metadata": {
        "id": "TmrEhWsIT6Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mistral-7B (Few-shot)**"
      ],
      "metadata": {
        "id": "LAMoQ31tYuvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/ZS_Test_modified_432.csv')\n",
        "\n",
        "llm_generated_templates = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    log_event = row['Content']\n",
        "    ground_truth = row['EventTemplate']\n",
        "    # log_event = row['Content'].strip('\"')\n",
        "    # ground_truth = row['EventTemplate'].strip('\"')\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "<|im_start|>system\n",
        "You are a log parsing assistant. Your task is to identify variable elements within the logs, generalize these elements, and construct a template that represents the structure of these log messages.\n",
        "<|im_end|>\n",
        "\n",
        "<|im_start|>user\n",
        "For your reference, here are a few log message examples and their corresponding templates.\n",
        "\n",
        "Example 1\n",
        "Log message: ```Error = , LOGIN chdir(/home/spelce1/UMT2K/umt2k/ckpt_umt2k_src/TEST/NEW_TEST) failed: No such file at /10.10.34.20:56374 point [/TEST/NEW_TEST], connect to proxy proxy.cse.cuhk.edu.hk:5070 to renew session (0x14f05578bd8001b)```\n",
        "Extracted template: ```Error = , LOGIN chdir(<*>) failed: No such file at <*>:<*> point [<*>], connect to proxy <*>:<*> to renew session (<*>)```\n",
        "\n",
        "Example 2\n",
        "Log message: ```ciod: In packet from nodes 91.0 and node-234 (R62-M1-Nf-C:J03-U11), message code 2 is not 3 or 4294967295 (softheader=003b005b 00030000 00000001 00000000)```\n",
        "Extracted template: ```ciod: In packet from nodes <*> and <*> (<*>:<*>), message code <*> is not <*> or <*> (softheader=<*> <*> <*> <*>)```\n",
        "\n",
        "Now, extract the template for the following log message. Replace any variable element with the placeholder '<*>'. Do not provide any explanation, only return the template.\n",
        "```{log_event}```\n",
        "<|im_end|>\n",
        "\n",
        "<|im_start|>assistant\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=60,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    print(\"========================================\")\n",
        "    print(f\"index: {index}\")\n",
        "    print(f\"Whole Output: {output}\")\n",
        "    print(\"========================================\")\n",
        "\n",
        "    output = output.replace('\"', '')\n",
        "    output = output.replace('\\n', '')\n",
        "\n",
        "    after_assistant = output.split(\"assistant\")[-1]\n",
        "\n",
        "    if \"<|im_end|>\" in after_assistant:\n",
        "        parsed_output = after_assistant.split(\"<|im_end|>\")[0].strip()\n",
        "    else:\n",
        "        parsed_output = after_assistant.strip()\n",
        "\n",
        "    llm_generated_templates.append(parsed_output)\n",
        "    print(\"----------------------------------------------\")\n",
        "    print(f\"Log Event: {log_event}\")\n",
        "    print(f\"Ground Truth: {ground_truth}\")\n",
        "    print(f\"Extracted Template: {parsed_output}\")\n",
        "    print(\"----------------------------------------------\")\n",
        "\n",
        "df['FsMistralExtractedTemplate'] = llm_generated_templates\n",
        "df.to_csv('/content/FS_ZS_Test_modified_432.csv', index=False)"
      ],
      "metadata": {
        "id": "qy9VKgwSY0L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mistal-7B (Fine-tuned)**\n",
        "\n"
      ],
      "metadata": {
        "id": "wZUQcmwTlf4A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRqkr-63s5Nz"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from torch.nn import DataParallel\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('unknown/Yarn-Mistral-7b-128k_Fine-Tuned4LogParsing-r2')\n",
        "model = AutoModelForCausalLM.from_pretrained('unknown/Yarn-Mistral-7b-128k_Fine-Tuned4LogParsing-r2')\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/FS_ZS_Test_modified_432.csv')\n",
        "\n",
        "llm_generated_templates = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    log_event = row['Content']\n",
        "    ground_truth = row['EventTemplate']\n",
        "    # log_event = row['Content'].strip('\"')\n",
        "    # ground_truth = row['EventTemplate'].strip('\"')\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "<|im_start|>system\n",
        "You are a log parsing assistant. Your task is to identify variable elements within the logs, generalize these elements, and construct a template that represents the structure of these log messages.\n",
        "<|im_end|>\n",
        "\n",
        "<|im_start|>user\n",
        "Extract the template for the following log message. Replace any variable element with the placeholder '<*>'. Do not provide any explanation, only return the template.\n",
        "```{log_event}```\n",
        "<|im_end|>\n",
        "\n",
        "<|im_start|>assistant\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=60,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    print(\"========================================\")\n",
        "    print(f\"index: {index}\")\n",
        "    print(f\"Whole Output: {output}\")\n",
        "    print(\"========================================\")\n",
        "\n",
        "    output = output.replace('\"', '')\n",
        "    output = output.replace('\\n', '')\n",
        "\n",
        "    after_assistant = output.split(\"assistant\")[-1]\n",
        "\n",
        "    if \"<|im_end|>\" in after_assistant:\n",
        "        parsed_output = after_assistant.split(\"<|im_end|>\")[0].strip()\n",
        "    else:\n",
        "        parsed_output = after_assistant.strip()\n",
        "\n",
        "    llm_generated_templates.append(parsed_output)\n",
        "    print(\"----------------------------------------------\")\n",
        "    print(f\"Log Event: {log_event}\")\n",
        "    print(f\"Ground Truth: {ground_truth}\")\n",
        "    print(f\"Extracted Template: {parsed_output}\")\n",
        "    print(\"----------------------------------------------\")\n",
        "\n",
        "df['FTMistralExtractedTemplate'] = llm_generated_templates\n",
        "df.to_csv('/content/FT_FS_ZS_Test_modified_432.csv', index=False)"
      ],
      "metadata": {
        "id": "sZdwuKDRxfKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GPT4 Results**"
      ],
      "metadata": {
        "id": "stXfnENggqh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "6k0b_qvsOHEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "7K2MuNu7ZXnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "import random\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "3c7sNEQvfXRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPT-4 (Zero-shot)**"
      ],
      "metadata": {
        "id": "5HK6o2MUgxFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "def gpt_reponse(log_event, ground_truth):\n",
        "  # openai_api_key = ''\n",
        "\n",
        "  openai_api_key = ''\n",
        "\n",
        "  gpt4_prompt = \"\"\"\n",
        "You are a log parsing assistant. Your task is to identify variable elements within the logs, generalize these elements, and construct a template that represents the structure of these log messages.\n",
        "\n",
        "Extract the template for the following log message. Replace any variable element with the placeholder '<*>'. Do not provide any explanation, only return the template.\n",
        "```{log_event}```\n",
        "  \"\"\"\n",
        "\n",
        "  prompt = PromptTemplate(\n",
        "    input_variables = ['log_event'],\n",
        "    template = gpt4_prompt\n",
        "    )\n",
        "\n",
        "  prompt.format(log_event = log_event)\n",
        "\n",
        "  # llm = OpenAI(temperature=0, openai_api_key = openai_api_key)\n",
        "\n",
        "  llm = ChatOpenAI(model=\"gpt-4\", temperature=0, openai_api_key = openai_api_key)\n",
        "  chain = LLMChain(llm = llm, prompt = prompt)\n",
        "  response = chain({'log_event': log_event})\n",
        "  # print(response)\n",
        "  extracted_template = response.get('text', 'No Values')\n",
        "  # extracted_template = extracted_template.strip('```');\n",
        "  print('==================================================================')\n",
        "\n",
        "  print (f\"Log Event: {log_event}\")\n",
        "  print (f\"Ground Truth: {ground_truth}\")\n",
        "  print (f\"Extracted Template: {extracted_template}\")\n",
        "\n",
        "\n",
        "  return extracted_template\n",
        "\n",
        "\n",
        "\n",
        "# csv_file_path = '/content/samp.csv'\n",
        "csv_file_path = '/content/Test_modified_432.csv'\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "df['ZSGPT4ExtractedTemplate'] = df.apply(lambda row: gpt_reponse(row['Content'], row['EventTemplate']), axis=1)\n",
        "# df.to_csv('/content/GPT4Zh_FT_FS_ZS_Test_modified_432.csv', index=False)\n",
        "df.to_csv('/content/GPT4ZS_Test_modified_432.csv', index=False)\n",
        "\n",
        "print('==================================================================')\n",
        "print(\"Results added to the CSV file.\")\n"
      ],
      "metadata": {
        "id": "bylwr44bEBCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPT-4 (Few-shot)**"
      ],
      "metadata": {
        "id": "EhA3tbHMgW4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "def gpt_reponse(log_event, ground_truth):\n",
        "  # openai_api_key = ''\n",
        "\n",
        "  openai_api_key = ''\n",
        "\n",
        "  gpt4_prompt = \"\"\"\n",
        "You are a log parsing assistant. Your task is to identify variable elements within the logs, generalize these elements, and construct a template that represents the structure of these log messages.\n",
        "\n",
        "For your reference, here are a few log message examples and their corresponding templates.\n",
        "\n",
        "Example 1\n",
        "Log message: ```Error = , LOGIN chdir(/home/spelce1/UMT2K/umt2k/ckpt_umt2k_src/TEST/NEW_TEST) failed: No such file at /10.10.34.20:56374 point [/TEST/NEW_TEST], connect to proxy proxy.cse.cuhk.edu.hk:5070 to renew session (0x14f05578bd8001b)```\n",
        "Extracted template: ```Error = , LOGIN chdir(<*>) failed: No such file at <*>:<*> point [<*>], connect to proxy <*>:<*> to renew session (<*>)```\n",
        "\n",
        "Example 2\n",
        "Log message: ```ciod: In packet from nodes 91.0 and node-234 (R62-M1-Nf-C:J03-U11), message code 2 is not 3 or 4294967295 (softheader=003b005b 00030000 00000001 00000000)```\n",
        "Extracted template: ```ciod: In packet from nodes <*> and <*> (<*>:<*>), message code <*> is not <*> or <*> (softheader=<*> <*> <*> <*>)```\n",
        "\n",
        "Now, extract the template for the following log message. Replace any variable element with the placeholder '<*>'. Do not provide any explanation, only return the template.\n",
        "```{log_event}```\n",
        "  \"\"\"\n",
        "\n",
        "  prompt = PromptTemplate(\n",
        "    input_variables = ['log_event'],\n",
        "    template = gpt4_prompt\n",
        "    )\n",
        "\n",
        "  prompt.format(log_event = log_event)\n",
        "\n",
        "  # llm = OpenAI(temperature=0, openai_api_key = openai_api_key)\n",
        "  llm = ChatOpenAI(model=\"gpt-4\", temperature=0, openai_api_key = openai_api_key)\n",
        "  chain = LLMChain(llm = llm, prompt = prompt)\n",
        "  response = chain({'log_event': log_event})\n",
        "\n",
        "  extracted_template = response.get('text', 'No Values')\n",
        "\n",
        "  print('==================================================================')\n",
        "\n",
        "  print (f\"Log Event: {log_event}\")\n",
        "  print (f\"Ground Truth: {ground_truth}\")\n",
        "  print (f\"Extracted Template: {extracted_template}\")\n",
        "\n",
        "\n",
        "  return extracted_template\n",
        "\n",
        "\n",
        "\n",
        "#csv_file_path = '/content/evaltest.csv'\n",
        "csv_file_path = '/content/Test_modified_432.csv'\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "df['FSGPT4ExtractedTemplate'] = df.apply(lambda row: gpt_reponse(row['Content'], row['EventTemplate']), axis=1)\n",
        "df.to_csv('/content/GPT4FS_Test_modified_432.csv', index=False)\n",
        "\n",
        "print('==================================================================')\n",
        "print(\"Results added to the CSV file.\")\n"
      ],
      "metadata": {
        "id": "c76b7NGlg9lR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GPT4 as Evaluator**"
      ],
      "metadata": {
        "id": "8iboyuCAX07P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "import re\n",
        "import time\n",
        "\n",
        "def evaluator(content, event_template, generated_template):\n",
        "  # openai_api_key = ''\n",
        "\n",
        "  c= content\n",
        "  et = event_template\n",
        "  gt = generated_template\n",
        "\n",
        "  openai_api_key = ''\n",
        "\n",
        "  eval_prompt = \"\"\"\n",
        "Task Description:\n",
        "Log parsing is the process of converting unstructured log messages into structured event templates by extracting the constant and variable parts of raw logs.\n",
        "You are a log parsing evaluator that assesses the quality of an extracted log template from a log message, given a ground truth template.\n",
        "\n",
        "Evaluation Criteria:\n",
        "Your evaluation must cover the accuracy and robustness of the extracted template.\n",
        "- Accuracy: This gauges the fidelity of the extracted templates in capturing the constant and variable parts of log messages, as defined by the ground truth.\n",
        "Accuracy is measured by the degree to which the extracted template aligns with the ground truth.\n",
        "- Robustness: This gauges the adaptability of the extracted templates across varied log types, including those with noise, inconsistencies, or diverse structures.\n",
        "Robustness is measured by the degree to which the extracted template is capable of preserving its quality and representation across diverse logs.\n",
        "\n",
        "Scoring:\n",
        "You will explain your reasoning and give a score between 0 and 5 for each evaluation criterion.\n",
        "A score of 0 means the extracted template is completely wrong or unusable, while a score of 5 means the extracted template is perfect or optimal.\n",
        "Assign a score of 5 for both Accuracy and Robustness if the extracted template matches the ground truth template exactly.\n",
        "\n",
        "Examples:\n",
        "For your reference, here are a few evaluation examples.\n",
        "\n",
        "- Example 1\n",
        "Log message: ```Client attempting to renew session (0x14f05578bd8001b) at (/10.10.34.20:56374)```\n",
        "Extracted template: ```Client attempting to renew session <*> at <*>```\n",
        "Ground truth template: ```Client attempting to renew session (<*>) at (<*>:<*>)```\n",
        "\n",
        "Evaluation:\n",
        "- Accuracy: The extracted template simplifies the variable parts into two placeholders <*>, which corresponds to the session ID and the IP:port pair in the log message.\n",
        "  However, it does not accurately reflect the structure of the variable parts as indicated in the ground truth template. The ground truth specifies three separate placeholders\n",
        "  within parentheses and a distinct separation for the IP address and port, suggesting a more granular level of detail. Accuracy score: 3.\n",
        "- Robustness: The extracted template's simplified approach to variable parts (<*> at <*>) could be seen as more adaptable to a wider range of log messages, as it does not strictly enforce the format of the variable parts.\n",
        "  This could potentially make the template more robust when dealing with logs that have minor deviations from the expected format, such as missing parentheses or variations in how the session ID and the IP:port pair are presented. Robustness score: 5.\n",
        "\n",
        "- Example 2\n",
        "Log message: ```Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742```\n",
        "Extracted template: ```Unknown base file: /var/lib/nova/instances/_base/<*>```\n",
        "Ground truth template: ```Unknown base file: <*>```\n",
        "\n",
        "Evaluation:\n",
        "- Accuracy: The extracted template accurately represents the constant part of the log message up to /var/lib/nova/instances/_base/, and correctly identifies the variable part as <*>.\n",
        "  However, it does not fully align with the ground truth's approach of considering the entire path as variable.  Accuracy score: 4.\n",
        "- Robustness: The extracted template specifies a particular path structure (/var/lib/nova/instances/_base/) before introducing the variable component.\n",
        "  This specificity might limit its adaptability across varied log messages that do not follow this exact path structure but still pertain to unknown base files. Robustness score: 3.\n",
        "\n",
        "Now, evaluate the accuracy and robustness of the extracted template for the following log message given the ground truth template. Explain your reasoning and give a score between 0 and 5 for each evaluation criterion.\n",
        "Log message: ```{log_event}```\n",
        "Extracted template: ```{generated_template}```\n",
        "Ground truth template: ```{ground_truth}```\n",
        "\"\"\"\n",
        "\n",
        "  prompt = PromptTemplate(\n",
        "    input_variables = ['log_event', 'ground_truth', 'generated_template'],\n",
        "    template = eval_prompt\n",
        "    )\n",
        "\n",
        "  prompt.format(log_event = content, ground_truth = event_template, generated_template = generated_template)\n",
        "\n",
        "  #llm = OpenAI(temperature=0, openai_api_key = openai_api_key)\n",
        "  llm = ChatOpenAI(model=\"gpt-4\", temperature=0,openai_api_key = openai_api_key)\n",
        "\n",
        "  # print(llm.model_name)\n",
        "  print('---------------------------------------------------')\n",
        "  chain = LLMChain(llm = llm, prompt = prompt)\n",
        "  response = chain({'log_event': content, 'ground_truth': event_template, 'generated_template': generated_template})\n",
        "  # print (f\"Whole response: {response}\")\n",
        "\n",
        "  metrics = response.get('text', 'No Values')\n",
        "\n",
        "\n",
        "  accuracy_match = re.search(r\"Accuracy score: (\\d+(\\.\\d+)?)\\.\", metrics)\n",
        "  accuracy_score = accuracy_match.group(1) if accuracy_match else None\n",
        "\n",
        "  accuracy_description_match = re.search(r\"- Accuracy: (.*?) Accuracy score:\", metrics, re.DOTALL)\n",
        "  accuracy_description = accuracy_description_match.group(1).strip() if accuracy_description_match else None\n",
        "\n",
        "\n",
        "\n",
        "  robustness_match = re.search(r\"Robustness score: (\\d+(\\.\\d+)?)\\.\", metrics)\n",
        "  robustness_score = robustness_match.group(1) if robustness_match else None\n",
        "\n",
        "  robustness_description_match = re.search(r\"- Robustness: (.*?) Robustness score:\", metrics, re.DOTALL)\n",
        "  robustness_description = robustness_description_match.group(1).strip() if robustness_description_match else None\n",
        "\n",
        "\n",
        "  print(f\"Accuracy score: {accuracy_score}\")\n",
        "  print(f\"Accuracy reasoning: {accuracy_description}\")\n",
        "  print(f\"Robustness score: {robustness_score}\")\n",
        "  print(f\"Robustness reasoning: {robustness_description}\")\n",
        "  #print(f\"Score: {score}\")\n",
        "\n",
        "  return accuracy_score, accuracy_description, robustness_score, robustness_description\n",
        "\n",
        "\n",
        "\n",
        "input_csv_file_path = '/content/GPT4EvalZSMistral.csv'\n",
        "df = pd.read_csv(input_csv_file_path)\n",
        "\n",
        "evaluation_results = []\n",
        "for index, row in df.iterrows():\n",
        "    print(index)\n",
        "    result = evaluator(row['Content'], row['EventTemplate'], row['ZsMistralExtractedTemplate'])\n",
        "    # accuracy_score, accuracy_description, robustness_score, robustness_description = result\n",
        "    # if accuracy_score is None or accuracy_description is None or robustness_score is None or robustness_description is None:\n",
        "      # print('None')\n",
        "    # result = evaluator(row['Content'], row['EventTemplate'], row['ZsMistralExtractedTemplate'])\n",
        "    # else:\n",
        "    evaluation_results.append(result)\n",
        "\n",
        "    # After processing each row, wait for 20 seconds before continuing to the next one\n",
        "    # This limits the script to 3 requests per minute\n",
        "    # time.sleep(30)\n",
        "\n",
        "df[['ZSMist_Acc', 'ZSMist_Acc_despn', 'ZSMist_Robust', 'ZSMist_Robust_despn']] = pd.DataFrame(evaluation_results, index=df.index)\n",
        "\n",
        "output_csv_file_path = '/content/GPT4EvalZSMistral.csv'\n",
        "df.to_csv(output_csv_file_path, index=False)\n"
      ],
      "metadata": {
        "id": "M1jWKAJ5X5JR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
